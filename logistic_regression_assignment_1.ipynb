{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q1. Difference Between Linear Regression and Logistic Regression\n",
    "\n",
    "\t•\tLinear Regression: This model is used for predicting continuous numeric values. It fits a straight line through the data points by minimizing the sum of squared errors. For example, predicting house prices based on variables like square footage and location.\n",
    "\t•\tLogistic Regression: This model is used for classification tasks, particularly binary classification (e.g., 0 or 1). It predicts the probability of a binary outcome using a sigmoid function that maps predictions to values between 0 and 1. For example, predicting whether an email is spam (1) or not spam (0).\n",
    "\n",
    "Scenario for Logistic Regression:\n",
    "If you are building a model to determine whether a patient has a particular disease based on diagnostic features, logistic regression would be appropriate because the outcome is binary (disease/no disease).\n",
    "\n",
    "Q2. Cost Function in Logistic Regression\n",
    "\n",
    "\t•\tThe cost function used in logistic regression is the Log-Loss (Binary Cross-Entropy). Unlike the Mean Squared Error used in linear regression, log-loss penalizes wrong predictions based on their probabilities, encouraging the model to make confident and accurate predictions.\n",
    "\t•\tOptimization: The cost function is optimized using Gradient Descent, where the model iteratively adjusts weights to minimize the loss.\n",
    "\n",
    "Q3. Regularization in Logistic Regression\n",
    "\n",
    "\t•\tRegularization adds a penalty to the cost function to avoid overfitting by constraining the size of the model coefficients.\n",
    "\t•\tTwo common forms of regularization:\n",
    "\t1.\tL1 Regularization (Lasso): Adds a penalty equal to the absolute value of the coefficients, encouraging some coefficients to be zero, effectively performing feature selection.\n",
    "\t2.\tL2 Regularization (Ridge): Adds a penalty equal to the square of the coefficients, preventing any one feature from dominating the model.\n",
    "\t•\tRegularization helps the model generalize better to new data by discouraging complex models that fit the noise in the training data.\n",
    "\n",
    "Q4. ROC Curve and Model Evaluation\n",
    "\n",
    "\t•\tROC (Receiver Operating Characteristic) Curve: It plots the True Positive Rate (TPR) (sensitivity) against the False Positive Rate (FPR) (1 - specificity) at various threshold levels.\n",
    "\t•\tAUC (Area Under the Curve): The area under the ROC curve (AUC) measures the model’s ability to distinguish between positive and negative classes. An AUC of 1 indicates a perfect classifier, while an AUC of 0.5 suggests the model performs no better than random guessing.\n",
    "\n",
    "Q5. Feature Selection Techniques in Logistic Regression\n",
    "\n",
    "\t•\tCommon Techniques:\n",
    "\t1.\tRecursive Feature Elimination (RFE): Repeatedly fits the model, removes the least important features, and refits.\n",
    "\t2.\tL1 Regularization (Lasso): Can automatically set coefficients of less important features to zero.\n",
    "\t3.\tStatistical Tests: Chi-square tests, correlation coefficients, and ANOVA can help determine which features are significant.\n",
    "\t•\tBenefits: Feature selection reduces overfitting, speeds up model training, and improves model interpretability by focusing on the most informative features.\n",
    "\n",
    "Q6. Handling Imbalanced Datasets in Logistic Regression\n",
    "\n",
    "\t•\tStrategies for Handling Class Imbalance:\n",
    "\t1.\tResampling Techniques:\n",
    "\t•\tOversampling the Minority Class: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) generate synthetic samples for the minority class to balance the dataset.\n",
    "\t•\tUndersampling the Majority Class: Reducing the number of samples from the majority class to balance the classes.\n",
    "\t2.\tAdjusting Class Weights: Modify the loss function to assign higher penalties for misclassifying the minority class, ensuring the model learns to pay more attention to it.\n",
    "\t3.\tUse of Advanced Models: Models like ensemble methods (Random Forest, Gradient Boosting) can handle imbalanced data better.\n",
    "\n",
    "Q7. Common Issues in Logistic Regression and Solutions\n",
    "\n",
    "\t1.\tMulticollinearity:\n",
    "\t•\tOccurs when two or more independent variables are highly correlated, leading to unstable coefficient estimates.\n",
    "\t•\tSolution: Use Variance Inflation Factor (VIF) to detect multicollinearity and remove or combine correlated features. Alternatively, apply Principal Component Analysis (PCA) to reduce dimensionality.\n",
    "\t2.\tOverfitting:\n",
    "\t•\tOccurs when the model learns patterns from the noise in the training data, performing poorly on new data.\n",
    "\t•\tSolution: Use regularization (L1 or L2), simplify the model by reducing the number of features, or gather more training data.\n",
    "\t3.\tClass Imbalance:\n",
    "\t•\tThe model may be biased toward the majority class, leading to poor performance in detecting the minority class.\n",
    "\t•\tSolution: Apply resampling techniques, adjust class weights, or use models specifically designed to handle imbalanced datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
